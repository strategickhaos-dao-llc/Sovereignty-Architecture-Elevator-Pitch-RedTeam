# Category 01: Computer Science & AI Foundations

This category contains 10 foundational papers that define modern artificial intelligence and deep learning.

## Papers Included

1. **Attention Is All You Need** - Vaswani et al. (2017)
   - The Transformer architecture that revolutionized NLP
   - Foundation for GPT, BERT, and modern LLMs

2. **GPT-3: Language Models are Few-Shot Learners** - Brown et al. (2020)
   - Demonstrated emergent capabilities at scale
   - Few-shot learning without fine-tuning

3. **Word2Vec: Efficient Estimation of Word Representations** - Mikolov et al. (2013)
   - Word embeddings that capture semantic relationships
   - Foundation for modern NLP

4. **BERT: Pre-training of Deep Bidirectional Transformers** - Devlin et al. (2018)
   - Bidirectional context understanding
   - Breakthrough in transfer learning for NLP

5. **Neural Machine Translation** - Bahdanau et al. (2014)
   - Attention mechanism for sequence-to-sequence models
   - Precursor to Transformers

6. **Deep Residual Learning (ResNet)** - He et al. (2015)
   - Skip connections enabling very deep networks
   - Revolution in computer vision

7. **Generative Adversarial Networks (GANs)** - Goodfellow et al. (2014)
   - Adversarial training for generative models
   - Foundation for image generation

8. **Auto-Encoding Variational Bayes (VAE)** - Kingma & Welling (2013)
   - Probabilistic generative models
   - Latent space representations

9. **Wasserstein GAN** - Arjovsky et al. (2017)
   - Improved GAN training stability
   - Better theoretical foundation

## Why These Papers Matter

These papers represent the core innovations that power modern AI:
- **Transformers** are the backbone of GPT, Claude, and every major LLM
- **Word embeddings** enable semantic understanding
- **GANs and VAEs** power generative AI
- **ResNet** made deep learning practically viable

Understanding these foundations is essential for:
- Building AI systems
- Evaluating AI claims
- Contributing to AI research
- Maintaining AI sovereignty

## Recommended Reading Order

1. Start with **Word2Vec** to understand embeddings
2. Read **ResNet** to grasp deep learning fundamentals
3. Study **Attention Is All You Need** as the modern foundation
4. Read **BERT** and **GPT-3** to see transformers in action
5. Explore **GANs** and **VAEs** for generative AI

## Related Categories

- **06_mathematics_formal**: Mathematical foundations of these techniques
- **08_energy_hardware**: Hardware requirements for training these models
- **07_licenses_ethics**: Ethical considerations for AI deployment
