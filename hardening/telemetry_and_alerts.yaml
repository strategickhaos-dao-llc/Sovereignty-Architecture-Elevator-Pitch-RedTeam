# Telemetry and Alerts Configuration
# Strategickhaos DAO LLC - Risk Mitigation Framework
# Version: 1.0.0
# Generated: 2025-11-25
#
# SLO definitions and alerting thresholds for all hardening domains

telemetry:
  version: "1.0.0"
  operator: "Domenic Garza (Node 137)"
  
# Defensive Publication SLOs
defensive_publication_slo:
  description: "Service Level Objectives for defensive publication pipeline"
  
  metrics:
    timestamp_failures:
      target: "< 1%"
      measurement: "failed_timestamps / total_timestamps * 100"
      window: "7d rolling"
      
    ghost_proofs:
      target: "0 per week"
      measurement: "count of incomplete merges in auto-stamp workflow"
      window: "7d"
      
    disclosure_validation_pass_rate:
      target: "> 99%"
      measurement: "passed_validations / total_validations * 100"
      window: "30d rolling"
      
    encryption_coverage:
      target: "100%"
      measurement: "encrypted_artifacts / total_artifacts * 100"
      window: "continuous"
      
    patent_review_completion:
      target: "> 95%"
      measurement: "reviewed_disclosures / flagged_disclosures * 100"
      window: "30d rolling"

# Spark/Copilot Swarm SLOs
spark_swarm_slo:
  description: "Service Level Objectives for Spark app development and Copilot agents"
  
  metrics:
    preview_crash_rate:
      target: "< 2%"
      measurement: "crashed_previews / total_previews * 100"
      window: "7d rolling"
      
    spend_per_preview_p95:
      target: "< $20"
      measurement: "95th percentile of USD spend per preview session"
      window: "7d rolling"
      
    dependency_conflict_rate:
      target: "< 1%"
      measurement: "builds_with_conflicts / total_builds * 100"
      window: "30d rolling"
      
    ai_code_bug_rate:
      target: "< 5%"
      measurement: "ai_generated_bugs_detected / total_ai_code_reviews * 100"
      window: "30d rolling"
      
    agent_session_timeout_rate:
      target: "< 3%"
      measurement: "timed_out_sessions / total_sessions * 100"
      window: "7d rolling"

# AI Governance SLOs
ai_governance_slo:
  description: "Service Level Objectives for AI agent governance"
  
  metrics:
    loop_rate:
      target: "< 0.5%"
      measurement: "detected_loops / total_agent_executions * 100"
      window: "7d rolling"
      
    mean_time_to_circuit_break:
      target: "< 2 minutes"
      measurement: "average time from anomaly detection to circuit break"
      window: "30d rolling"
      
    ethics_violation_rate:
      target: "0%"
      measurement: "ethics_violations / total_outputs * 100"
      window: "30d rolling"
      
    regulatory_compliance_rate:
      target: "> 99%"
      measurement: "compliant_operations / total_operations * 100"
      window: "quarterly"
      
    data_quality_score:
      target: "> 0.95"
      measurement: "weighted average of input validation scores"
      window: "7d rolling"
      
    model_drift_score:
      target: "< 0.1"
      measurement: "deviation from baseline benchmark performance"
      window: "7d rolling"

# Alerting Thresholds and Actions
alerting:
  channels:
    pager:
      type: pagerduty
      severity: critical
      
    slack:
      type: slack
      channel: "#alerts-hardening"
      severity: [critical, warning]
      
    email:
      type: email
      recipients: ["domenic.garza@snhu.edu"]
      severity: [critical, warning, info]
      
    github_issue:
      type: github
      repo: "Strategickhaos/Sovereignty-Architecture-Elevator-Pitch-"
      labels: ["alert", "hardening"]
      severity: [warning, info]
      
  rules:
    # Defensive Publication Alerts
    - name: ghost_proof_detected
      condition: "ghost_proofs > 0"
      severity: critical
      action: page
      message: "Ghost proof detected - incomplete merge in auto-stamp workflow"
      runbook: "Check recent merges, verify artifact hashes, re-run validation"
      
    - name: timestamp_failure_spike
      condition: "timestamp_failures > 3 in 1h"
      severity: warning
      action: notify
      message: "Multiple timestamp failures detected"
      runbook: "Check OpenTimestamps API, verify Bitcoin network status, try mirrors"
      
    - name: disclosure_leak
      condition: "github_secret_scan.findings > 0"
      severity: critical
      action: page_and_freeze
      message: "Sensitive data exposure detected in disclosure"
      runbook: "Immediately revoke exposed credentials, audit access logs"
      
    # Spark/Copilot Alerts
    - name: cost_spike
      condition: "current_spend > 3 * baseline_spend"
      severity: critical
      action: page
      message: "Unusual spend detected - possible runaway agent"
      runbook: "Check active previews, terminate suspicious sessions"
      
    - name: loop_detected
      condition: "loop_iterations > 20"
      severity: critical
      action: circuit_break
      message: "Agent loop detected - circuit breaker triggered"
      runbook: "Review agent logs, identify loop cause, update failure library"
      
    - name: dependency_vulnerability
      condition: "dependabot.critical_vulnerabilities > 0"
      severity: high
      action: block_deploy
      message: "Critical vulnerability in dependencies"
      runbook: "Update dependencies, run security scan, verify fix"
      
    # AI Governance Alerts
    - name: decay_score_critical
      condition: "decay_score < 0.6"
      severity: critical
      action: page
      message: "Model/system decay below threshold"
      runbook: "Run benchmark suite, compare to baseline, investigate drift"
      
    - name: ethics_violation
      condition: "ethics_check.violations > 0"
      severity: critical
      action: dao_veto
      message: "Ethics violation detected - DAO veto path triggered"
      runbook: "Review output, assess violation type, follow dao_veto_path playbook"
      
    - name: cascade_cost_exceeded
      condition: "multi_agent_cost > $5"
      severity: warning
      action: throttle
      message: "Multi-agent cascade cost approaching limit"
      runbook: "Review inter-agent calls, check for loops, apply rate limits"

# Prometheus Metrics Export Configuration
prometheus:
  scrape_interval: "15s"
  evaluation_interval: "15s"
  
  metrics:
    # Defensive Publication
    - name: defpub_timestamp_total
      type: counter
      help: "Total number of timestamp operations"
      
    - name: defpub_timestamp_failures_total
      type: counter
      help: "Total number of failed timestamp operations"
      
    - name: defpub_ghost_proofs_total
      type: counter
      help: "Total number of ghost proofs detected"
      
    # Spark/Copilot
    - name: spark_preview_total
      type: counter
      help: "Total number of Spark previews"
      
    - name: spark_preview_crashes_total
      type: counter
      help: "Total number of crashed previews"
      
    - name: spark_spend_usd
      type: histogram
      help: "USD spend per preview session"
      buckets: [1, 5, 10, 20, 50, 100]
      
    # AI Governance
    - name: agent_executions_total
      type: counter
      help: "Total number of agent executions"
      
    - name: agent_loops_detected_total
      type: counter
      help: "Total number of detected agent loops"
      
    - name: governance_decay_score
      type: gauge
      help: "Current decay score (0-1)"
      
    - name: governance_ethics_violations_total
      type: counter
      help: "Total number of ethics violations"

# Grafana Dashboard IDs
grafana_dashboards:
  defensive_publication:
    id: "defpub-hardening-001"
    panels:
      - timestamp_success_rate
      - ghost_proof_timeline
      - disclosure_validation_status
      - encryption_coverage_gauge
      
  spark_copilot:
    id: "spark-hardening-001"
    panels:
      - preview_health
      - spend_distribution
      - dependency_status
      - agent_performance
      
  ai_governance:
    id: "governance-hardening-001"
    panels:
      - decay_score_timeline
      - ethics_compliance
      - regulatory_status
      - loop_detection_heatmap
