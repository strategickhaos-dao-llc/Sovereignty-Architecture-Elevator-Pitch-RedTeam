# docker-compose.legends.yml — Legends of Minds v1 (real, working, safe)
version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - F:/OllamaData/models:/root/.ollama/models   # ← your big empty drive
      - F:/OllamaData/tmp:/root/.ollama/tmp
    environment:
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_TMPDIR=/root/.ollama/tmp
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_MAX_LOADED_MODELS=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"   # web UI
    volumes:
      - F:/qdrant_storage:/qdrant/storage

  legends-control-center:
    image: python:3.12-slim
    container_name: legends-control-center
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - C:/legends_of_minds:/app
    working_dir: /app/core
    command: >
      sh -c "pip install fastapi uvicorn[standard] httpx psutil && 
             uvicorn main:app --host 0.0.0.0 --port 8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - ollama

  legends-ingest:
    image: python:3.12-slim
    container_name: legends-ingest
    restart: unless-stopped
    ports:
      - "8001:8000"
    volumes:
      - C:/legends_of_minds:/app
      - F:/uploads:/uploads
    working_dir: /app/ingest
    command: >
      sh -c "pip install fastapi uvicorn httpx && 
             uvicorn main:app --host 0.0.0.0 --port 8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - ollama
      - qdrant

volumes:
  qdrant_storage:

networks:
  default:
    name: legends-net
