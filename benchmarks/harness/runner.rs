// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FlameLang Statistical Operations Benchmark Harness
// Test Runner and Metrics Collection
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

use std::collections::HashMap;
use std::time::{Duration, Instant};
use std::process::Command;

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// METRIC TYPES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#[derive(Debug, Clone)]
pub struct BenchmarkMetrics {
    pub execution_time_ns: u64,
    pub memory_footprint_bytes: u64,
    pub lines_of_code: usize,
    pub semantic_clarity_score: f64,
    pub compilation_time_ms: u64,
}

#[derive(Debug, Clone)]
pub struct BenchmarkResult {
    pub problem_id: String,
    pub language: Language,
    pub metrics: BenchmarkMetrics,
    pub output: Vec<u8>,
    pub validation: ValidationResult,
}

#[derive(Debug, Clone, PartialEq)]
pub enum Language {
    Cpp,
    Rust,
    Flame,
}

#[derive(Debug, Clone)]
pub enum ValidationResult {
    Pass,
    Fail { reason: String },
    SemanticMismatch { details: String },
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// BENCHMARK RUNNER
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

pub struct BenchmarkRunner {
    problems: Vec<ProblemSpec>,
    results: Vec<BenchmarkResult>,
    config: RunnerConfig,
}

#[derive(Debug, Clone)]
pub struct ProblemSpec {
    pub id: String,
    pub name: String,
    pub category: String,
    pub section: String,
    pub cpp_path: Option<String>,
    pub rust_path: Option<String>,
    pub flame_path: Option<String>,
}

#[derive(Debug, Clone)]
pub struct RunnerConfig {
    pub iterations: usize,
    pub warmup_iterations: usize,
    pub enable_gdss_validation: bool,
    pub output_dir: String,
}

impl Default for RunnerConfig {
    fn default() -> Self {
        RunnerConfig {
            iterations: 10,
            warmup_iterations: 3,
            enable_gdss_validation: true,
            output_dir: "benchmarks/results".to_string(),
        }
    }
}

impl BenchmarkRunner {
    pub fn new(config: RunnerConfig) -> Self {
        BenchmarkRunner {
            problems: Vec::new(),
            results: Vec::new(),
            config,
        }
    }

    /// Load problem specifications from YAML
    pub fn load_problems(&mut self, yaml_path: &str) -> Result<(), String> {
        // TODO: Parse benchmark_problems.yaml
        // Extract problem definitions and construct ProblemSpec instances
        println!("Loading problems from: {}", yaml_path);
        Ok(())
    }

    /// Run all benchmarks for all languages
    pub fn run_all(&mut self) -> Result<(), String> {
        println!("Starting benchmark suite...");
        
        for problem in &self.problems {
            println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            println!("Running: {} ({})", problem.name, problem.id);
            println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
            
            // Run C++ baseline
            if let Some(cpp_path) = &problem.cpp_path {
                let result = self.run_cpp_benchmark(&problem.id, cpp_path)?;
                self.results.push(result);
            }
            
            // Run Rust comparison
            if let Some(rust_path) = &problem.rust_path {
                let result = self.run_rust_benchmark(&problem.id, rust_path)?;
                self.results.push(result);
            }
            
            // Run FlameLang implementation
            if let Some(flame_path) = &problem.flame_path {
                let result = self.run_flame_benchmark(&problem.id, flame_path)?;
                self.results.push(result);
            }
            
            // Validate semantic equivalence (GDSS)
            if self.config.enable_gdss_validation {
                self.validate_gdss(&problem.id)?;
            }
        }
        
        println!("\nâœ“ All benchmarks complete");
        Ok(())
    }

    /// Run C++ baseline benchmark
    fn run_cpp_benchmark(&self, problem_id: &str, path: &str) -> Result<BenchmarkResult, String> {
        println!("  [C++] Compiling...");
        
        // Compile
        let compile_start = Instant::now();
        let compile_output = Command::new("g++")
            .args(&["-std=c++17", "-O3", "-o", "/tmp/cpp_bench", path])
            .output()
            .map_err(|e| format!("Failed to compile C++: {}", e))?;
        let compilation_time_ms = compile_start.elapsed().as_millis() as u64;
        
        if !compile_output.status.success() {
            return Err(format!("C++ compilation failed: {:?}", compile_output.stderr));
        }
        
        // Run with timing
        println!("  [C++] Executing...");
        let exec_start = Instant::now();
        let exec_output = Command::new("/tmp/cpp_bench")
            .output()
            .map_err(|e| format!("Failed to execute C++: {}", e))?;
        let execution_time_ns = exec_start.elapsed().as_nanos() as u64;
        
        // Collect metrics
        let metrics = BenchmarkMetrics {
            execution_time_ns,
            memory_footprint_bytes: 0, // TODO: measure actual memory usage
            lines_of_code: count_lines_of_code(path),
            semantic_clarity_score: 0.0, // TODO: implement clarity scoring
            compilation_time_ms,
        };
        
        Ok(BenchmarkResult {
            problem_id: problem_id.to_string(),
            language: Language::Cpp,
            metrics,
            output: exec_output.stdout,
            validation: ValidationResult::Pass, // TODO: actual validation
        })
    }

    /// Run Rust comparison benchmark
    fn run_rust_benchmark(&self, problem_id: &str, path: &str) -> Result<BenchmarkResult, String> {
        println!("  [Rust] Compiling...");
        
        // Compile
        let compile_start = Instant::now();
        let compile_output = Command::new("rustc")
            .args(&["-O", "-o", "/tmp/rust_bench", path])
            .output()
            .map_err(|e| format!("Failed to compile Rust: {}", e))?;
        let compilation_time_ms = compile_start.elapsed().as_millis() as u64;
        
        if !compile_output.status.success() {
            return Err(format!("Rust compilation failed: {:?}", compile_output.stderr));
        }
        
        // Run with timing
        println!("  [Rust] Executing...");
        let exec_start = Instant::now();
        let exec_output = Command::new("/tmp/rust_bench")
            .output()
            .map_err(|e| format!("Failed to execute Rust: {}", e))?;
        let execution_time_ns = exec_start.elapsed().as_nanos() as u64;
        
        // Collect metrics
        let metrics = BenchmarkMetrics {
            execution_time_ns,
            memory_footprint_bytes: 0, // TODO: measure actual memory usage
            lines_of_code: count_lines_of_code(path),
            semantic_clarity_score: 0.0, // TODO: implement clarity scoring
            compilation_time_ms,
        };
        
        Ok(BenchmarkResult {
            problem_id: problem_id.to_string(),
            language: Language::Rust,
            metrics,
            output: exec_output.stdout,
            validation: ValidationResult::Pass, // TODO: actual validation
        })
    }

    /// Run FlameLang implementation benchmark
    fn run_flame_benchmark(&self, problem_id: &str, path: &str) -> Result<BenchmarkResult, String> {
        println!("  [FlameLang] Compiling...");
        
        // TODO: Implement FlameLang compiler integration
        // For now, placeholder implementation
        
        let metrics = BenchmarkMetrics {
            execution_time_ns: 0,
            memory_footprint_bytes: 0,
            lines_of_code: count_lines_of_code(path),
            semantic_clarity_score: 0.0,
            compilation_time_ms: 0,
        };
        
        Ok(BenchmarkResult {
            problem_id: problem_id.to_string(),
            language: Language::Flame,
            metrics,
            output: Vec::new(),
            validation: ValidationResult::Pass,
        })
    }

    /// Validate semantic equivalence using GDSS principles
    fn validate_gdss(&self, problem_id: &str) -> Result<(), String> {
        println!("  [GDSS] Validating semantic equivalence...");
        
        // Get results for this problem across all languages
        let problem_results: Vec<_> = self.results.iter()
            .filter(|r| r.problem_id == problem_id)
            .collect();
        
        if problem_results.len() < 2 {
            return Ok(()); // Need at least 2 implementations to compare
        }
        
        // TODO: Implement GDSS validation
        // - Compare visual output (if chart-based)
        // - Verify semantic structure equivalence
        // - Check that operations preserve meaning across languages
        
        println!("  [GDSS] âœ“ Semantic equivalence validated");
        Ok(())
    }

    /// Generate report comparing all implementations
    pub fn generate_report(&self) -> Result<String, String> {
        let mut report = String::new();
        
        report.push_str("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        report.push_str("FlameLang Statistical Operations Benchmark Report\n");
        report.push_str("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n");
        
        // Group results by problem
        let mut by_problem: HashMap<String, Vec<&BenchmarkResult>> = HashMap::new();
        for result in &self.results {
            by_problem.entry(result.problem_id.clone())
                .or_insert_with(Vec::new)
                .push(result);
        }
        
        // Generate comparison tables
        for (problem_id, results) in by_problem {
            report.push_str(&format!("\nProblem: {}\n", problem_id));
            report.push_str("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
            
            for result in results {
                let lang = match result.language {
                    Language::Cpp => "C++  ",
                    Language::Rust => "Rust ",
                    Language::Flame => "Flame",
                };
                
                report.push_str(&format!(
                    "{} | Time: {:>10}ns | Memory: {:>8}B | LOC: {:>4} | Compile: {:>6}ms\n",
                    lang,
                    result.metrics.execution_time_ns,
                    result.metrics.memory_footprint_bytes,
                    result.metrics.lines_of_code,
                    result.metrics.compilation_time_ms,
                ));
            }
        }
        
        report.push_str("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
        Ok(report)
    }

    /// Export results to JSON
    pub fn export_json(&self, path: &str) -> Result<(), String> {
        // TODO: Serialize results to JSON
        println!("Exporting results to: {}", path);
        Ok(())
    }

    /// Export results to CSV
    pub fn export_csv(&self, path: &str) -> Result<(), String> {
        // TODO: Serialize results to CSV
        println!("Exporting results to: {}", path);
        Ok(())
    }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// UTILITY FUNCTIONS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

fn count_lines_of_code(path: &str) -> usize {
    std::fs::read_to_string(path)
        .map(|content| content.lines().count())
        .unwrap_or(0)
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MAIN ENTRY POINT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#[cfg(not(test))]
fn main() -> Result<(), String> {
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("FlameLang Statistical Operations Benchmark Suite");
    println!("Session: LOM-2025-12-15-FINAL");
    println!("Codename: CONTRADICTION_TO_CREATION");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    let config = RunnerConfig::default();
    let mut runner = BenchmarkRunner::new(config);
    
    // Load problem specifications
    runner.load_problems("benchmarks/benchmark_problems.yaml")?;
    
    // Run all benchmarks
    runner.run_all()?;
    
    // Generate and display report
    let report = runner.generate_report()?;
    println!("\n{}", report);
    
    // Export results
    runner.export_json("benchmarks/results/results.json")?;
    runner.export_csv("benchmarks/results/results.csv")?;
    
    println!("\nğŸ”¥ Benchmark suite complete. Momentum preserved. ğŸ”¥");
    Ok(())
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TESTS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_runner_creation() {
        let config = RunnerConfig::default();
        let runner = BenchmarkRunner::new(config);
        assert_eq!(runner.problems.len(), 0);
        assert_eq!(runner.results.len(), 0);
    }

    #[test]
    fn test_count_lines_of_code() {
        // TODO: Add test for LOC counting
    }

    #[test]
    fn test_gdss_validation() {
        // TODO: Add GDSS validation tests
    }
}
