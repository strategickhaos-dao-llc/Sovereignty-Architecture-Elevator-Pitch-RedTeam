#!/usr/bin/env python3
"""
üìä PERFORMANCE CROSS-REFERENCE SYSTEM v1.0
Advanced Windows Performance Monitor Integration & Correlation Engine
Connects resmon data with network events, torrent activity, and resource consumption
"""

import os
import sys
import psutil
import subprocess
import json
import time
import threading
import queue
import sqlite3
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
import xml.etree.ElementTree as ET
import win32api
import win32con
import win32gui
import win32process
import wmi
import pythoncom

@dataclass
class PerformanceMetric:
    timestamp: datetime
    process_name: str
    pid: int
    cpu_percent: float
    memory_mb: float
    disk_reads: int
    disk_writes: int
    network_bytes_sent: int
    network_bytes_recv: int
    gpu_percent: Optional[float] = None
    handles: Optional[int] = None
    threads: Optional[int] = None

@dataclass
class SystemSnapshot:
    timestamp: datetime
    total_cpu_percent: float
    total_memory_mb: float
    total_memory_percent: float
    available_memory_mb: float
    disk_usage_percent: Dict[str, float]
    network_io_counters: Dict[str, int]
    active_processes: int
    system_uptime: timedelta
    performance_score: float

@dataclass
class CorrelationResult:
    metric_pair: Tuple[str, str]
    correlation_coefficient: float
    p_value: float
    confidence_interval: Tuple[float, float]
    relationship_strength: str
    interpretation: str

@dataclass
class PerformanceAnomaly:
    timestamp: datetime
    anomaly_type: str
    affected_metrics: List[str]
    severity_score: float
    description: str
    related_events: List[Dict]
    recommended_actions: List[str]

class PerformanceCrossReferenceSystem:
    def __init__(self, db_path: str = "sovereignty_performance.db"):
        self.db_path = db_path
        self.monitoring_active = False
        self.data_queue = queue.Queue()
        
        # Initialize database
        self._initialize_database()
        
        # Performance monitoring threads
        self.monitor_threads = []
        
        # Windows-specific components
        self.wmi_connection = None
        self.performance_counters = {}
        
        # Analysis components
        self.correlation_matrix = None
        self.anomaly_detector = IsolationForest(contamination=0.1, random_state=42)
        self.scaler = StandardScaler()
        
        # Cross-reference data stores
        self.network_events = []
        self.torrent_activity = []
        self.xcom_intelligence = []
        
        # Initialize Windows performance monitoring
        self._initialize_windows_monitoring()
    
    def _initialize_database(self):
        """Initialize SQLite database for performance data storage"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Performance metrics table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                process_name TEXT NOT NULL,
                pid INTEGER NOT NULL,
                cpu_percent REAL,
                memory_mb REAL,
                disk_reads INTEGER,
                disk_writes INTEGER,
                network_bytes_sent INTEGER,
                network_bytes_recv INTEGER,
                gpu_percent REAL,
                handles INTEGER,
                threads INTEGER
            )
        """)
        
        # System snapshots table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS system_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                total_cpu_percent REAL,
                total_memory_mb REAL,
                total_memory_percent REAL,
                available_memory_mb REAL,
                disk_usage_json TEXT,
                network_io_json TEXT,
                active_processes INTEGER,
                system_uptime_seconds INTEGER,
                performance_score REAL
            )
        """)
        
        # Correlations table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS correlations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                metric_pair TEXT NOT NULL,
                correlation_coefficient REAL,
                p_value REAL,
                confidence_interval TEXT,
                relationship_strength TEXT,
                interpretation TEXT
            )
        """)
        
        # Anomalies table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS anomalies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                anomaly_type TEXT NOT NULL,
                affected_metrics TEXT,
                severity_score REAL,
                description TEXT,
                related_events TEXT,
                recommended_actions TEXT
            )
        """)
        
        # Cross-reference events table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS cross_reference_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                event_type TEXT NOT NULL,
                source_system TEXT NOT NULL,
                event_data TEXT,
                correlation_id TEXT
            )
        """)
        
        conn.commit()
        conn.close()
    
    def _initialize_windows_monitoring(self):
        """Initialize Windows-specific performance monitoring"""
        try:
            # Initialize WMI connection for detailed Windows metrics
            pythoncom.CoInitialize()
            self.wmi_connection = wmi.WMI()
            
            # Set up performance counter queries
            self.performance_counters = {
                'processor': r'\\Processor(_Total)\\% Processor Time',
                'memory': r'\\Memory\\Available MBytes',
                'disk_reads': r'\\PhysicalDisk(_Total)\\Disk Reads/sec',
                'disk_writes': r'\\PhysicalDisk(_Total)\\Disk Writes/sec',
                'network_bytes_sent': r'\\Network Interface(*)\\Bytes Sent/sec',
                'network_bytes_recv': r'\\Network Interface(*)\\Bytes Received/sec'
            }
            
            print("‚úÖ Windows performance monitoring initialized")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Windows monitoring initialization warning: {e}")
            print("Falling back to cross-platform monitoring")
    
    def capture_windows_performance_data(self) -> Dict[str, Any]:
        """Capture detailed Windows performance data using WMI and performance counters"""
        try:
            data = {
                'timestamp': datetime.now(),
                'processes': [],
                'system_metrics': {}
            }
            
            # Get process information via WMI
            if self.wmi_connection:
                for process in self.wmi_connection.Win32_Process():
                    try:
                        # Get process performance counters
                        process_info = {
                            'name': process.Name,
                            'pid': process.ProcessId,
                            'creation_date': process.CreationDate,
                            'working_set_size': process.WorkingSetSize,
                            'page_file_usage': process.PageFileUsage,
                            'handle_count': process.HandleCount,
                            'thread_count': process.ThreadCount
                        }
                        
                        # Get CPU and I/O stats via psutil for more accurate real-time data
                        try:
                            psutil_process = psutil.Process(process.ProcessId)
                            process_info.update({
                                'cpu_percent': psutil_process.cpu_percent(),
                                'memory_percent': psutil_process.memory_percent(),
                                'io_counters': psutil_process.io_counters()._asdict() if psutil_process.io_counters() else {}
                            })
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            pass
                        
                        data['processes'].append(process_info)
                        
                    except Exception:
                        continue
            
            # Get system-wide metrics
            data['system_metrics'] = {
                'cpu_count': psutil.cpu_count(),
                'cpu_percent': psutil.cpu_percent(interval=1),
                'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
                'memory': psutil.virtual_memory()._asdict(),
                'swap': psutil.swap_memory()._asdict(),
                'disk_usage': {disk.device: psutil.disk_usage(disk.mountpoint)._asdict() 
                             for disk in psutil.disk_partitions() if 'cdrom' not in disk.opts},
                'disk_io': psutil.disk_io_counters()._asdict() if psutil.disk_io_counters() else {},
                'network_io': psutil.net_io_counters()._asdict(),
                'boot_time': psutil.boot_time(),
                'users': [user._asdict() for user in psutil.users()]
            }
            
            return data
            
        except Exception as e:
            print(f"Error capturing Windows performance data: {e}")
            return {}
    
    def capture_cross_platform_performance_data(self) -> Dict[str, Any]:
        """Capture performance data using cross-platform methods"""
        try:
            data = {
                'timestamp': datetime.now(),
                'processes': [],
                'system_metrics': {}
            }
            
            # Get all processes
            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_info', 'io_counters', 'num_threads', 'num_handles']):
                try:
                    proc_info = proc.info
                    
                    # Convert memory info
                    memory_info = proc_info.get('memory_info')
                    if memory_info:
                        proc_info['memory_mb'] = memory_info.rss / 1024 / 1024
                    
                    # Convert I/O counters
                    io_counters = proc_info.get('io_counters')
                    if io_counters:
                        proc_info['disk_reads'] = io_counters.read_count
                        proc_info['disk_writes'] = io_counters.write_count
                        proc_info['disk_read_bytes'] = io_counters.read_bytes
                        proc_info['disk_write_bytes'] = io_counters.write_bytes
                    
                    data['processes'].append(proc_info)
                    
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            # System metrics
            data['system_metrics'] = {
                'cpu_percent': psutil.cpu_percent(interval=1, percpu=True),
                'cpu_count': psutil.cpu_count(),
                'memory': psutil.virtual_memory()._asdict(),
                'disk_usage': {partition.device: psutil.disk_usage(partition.mountpoint)._asdict() 
                             for partition in psutil.disk_partitions()},
                'disk_io': psutil.disk_io_counters()._asdict() if psutil.disk_io_counters() else {},
                'network_io': psutil.net_io_counters()._asdict(),
                'network_connections': len(psutil.net_connections()),
                'boot_time': psutil.boot_time()
            }
            
            return data
            
        except Exception as e:
            print(f"Error capturing cross-platform performance data: {e}")
            return {}
    
    def start_performance_monitoring(self):
        """Start continuous performance monitoring"""
        self.monitoring_active = True
        
        # Start performance data collection thread
        perf_thread = threading.Thread(target=self._performance_monitor_loop, daemon=True)
        perf_thread.start()
        self.monitor_threads.append(perf_thread)
        
        # Start data processing thread
        proc_thread = threading.Thread(target=self._data_processing_loop, daemon=True)
        proc_thread.start()
        self.monitor_threads.append(proc_thread)
        
        print("üìä Performance monitoring started")
    
    def _performance_monitor_loop(self):
        """Main performance monitoring loop"""
        while self.monitoring_active:
            try:
                # Capture performance data
                if sys.platform == "win32" and self.wmi_connection:
                    perf_data = self.capture_windows_performance_data()
                else:
                    perf_data = self.capture_cross_platform_performance_data()
                
                # Queue data for processing
                self.data_queue.put(('performance', perf_data))
                
                # Wait before next capture
                time.sleep(5)  # 5-second intervals
                
            except Exception as e:
                print(f"Performance monitoring error: {e}")
                time.sleep(10)  # Longer delay on error
    
    def _data_processing_loop(self):
        """Process queued performance data"""
        while self.monitoring_active:
            try:
                # Get data from queue
                data_type, data = self.data_queue.get(timeout=1)
                
                if data_type == 'performance':
                    self._process_performance_data(data)
                elif data_type == 'network_event':
                    self._process_network_event(data)
                elif data_type == 'torrent_activity':
                    self._process_torrent_activity(data)
                elif data_type == 'xcom_intelligence':
                    self._process_xcom_intelligence(data)
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Data processing error: {e}")
    
    def _process_performance_data(self, perf_data: Dict):
        """Process and store performance data"""
        if not perf_data:
            return
        
        timestamp = perf_data['timestamp']
        
        # Store individual process metrics
        for process in perf_data.get('processes', []):
            metric = PerformanceMetric(
                timestamp=timestamp,
                process_name=process.get('name', 'Unknown'),
                pid=process.get('pid', 0),
                cpu_percent=process.get('cpu_percent', 0.0),
                memory_mb=process.get('memory_mb', 0.0),
                disk_reads=process.get('disk_reads', 0),
                disk_writes=process.get('disk_writes', 0),
                network_bytes_sent=0,  # Process-level network data not available via psutil
                network_bytes_recv=0,
                handles=process.get('num_handles', 0),
                threads=process.get('num_threads', 0)
            )
            
            self._store_performance_metric(metric)
        
        # Create system snapshot
        system_metrics = perf_data.get('system_metrics', {})
        memory_info = system_metrics.get('memory', {})
        disk_usage = system_metrics.get('disk_usage', {})
        network_io = system_metrics.get('network_io', {})
        
        # Calculate performance score
        performance_score = self._calculate_performance_score(system_metrics)
        
        snapshot = SystemSnapshot(
            timestamp=timestamp,
            total_cpu_percent=sum(system_metrics.get('cpu_percent', [0])) / len(system_metrics.get('cpu_percent', [1])),
            total_memory_mb=memory_info.get('total', 0) / 1024 / 1024,
            total_memory_percent=memory_info.get('percent', 0.0),
            available_memory_mb=memory_info.get('available', 0) / 1024 / 1024,
            disk_usage_percent={device: usage.get('percent', 0) for device, usage in disk_usage.items()},
            network_io_counters=network_io,
            active_processes=len(perf_data.get('processes', [])),
            system_uptime=timedelta(seconds=time.time() - system_metrics.get('boot_time', time.time())),
            performance_score=performance_score
        )
        
        self._store_system_snapshot(snapshot)
        
        # Trigger correlation analysis periodically
        if timestamp.minute % 5 == 0:  # Every 5 minutes
            self._trigger_correlation_analysis()
    
    def _calculate_performance_score(self, system_metrics: Dict) -> float:
        """Calculate overall performance score (0-100)"""
        try:
            memory_info = system_metrics.get('memory', {})
            cpu_percent = system_metrics.get('cpu_percent', [0])
            
            # Normalize metrics
            cpu_score = max(0, 100 - sum(cpu_percent) / len(cpu_percent))
            memory_score = max(0, 100 - memory_info.get('percent', 0))
            
            # Disk performance (simplified)
            disk_score = 90  # Assume good disk performance unless we detect issues
            
            # Network performance (simplified)
            network_score = 95  # Assume good network performance
            
            # Weighted average
            overall_score = (
                cpu_score * 0.4 +
                memory_score * 0.3 +
                disk_score * 0.2 +
                network_score * 0.1
            )
            
            return round(overall_score, 2)
            
        except Exception:
            return 50.0  # Default score on error
    
    def _store_performance_metric(self, metric: PerformanceMetric):
        """Store performance metric in database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO performance_metrics 
            (timestamp, process_name, pid, cpu_percent, memory_mb, disk_reads, disk_writes,
             network_bytes_sent, network_bytes_recv, gpu_percent, handles, threads)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            metric.timestamp.isoformat(),
            metric.process_name,
            metric.pid,
            metric.cpu_percent,
            metric.memory_mb,
            metric.disk_reads,
            metric.disk_writes,
            metric.network_bytes_sent,
            metric.network_bytes_recv,
            metric.gpu_percent,
            metric.handles,
            metric.threads
        ))
        
        conn.commit()
        conn.close()
    
    def _store_system_snapshot(self, snapshot: SystemSnapshot):
        """Store system snapshot in database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO system_snapshots
            (timestamp, total_cpu_percent, total_memory_mb, total_memory_percent,
             available_memory_mb, disk_usage_json, network_io_json, active_processes,
             system_uptime_seconds, performance_score)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            snapshot.timestamp.isoformat(),
            snapshot.total_cpu_percent,
            snapshot.total_memory_mb,
            snapshot.total_memory_percent,
            snapshot.available_memory_mb,
            json.dumps(snapshot.disk_usage_percent),
            json.dumps(snapshot.network_io_counters),
            snapshot.active_processes,
            int(snapshot.system_uptime.total_seconds()),
            snapshot.performance_score
        ))
        
        conn.commit()
        conn.close()
    
    def add_network_event(self, event_data: Dict):
        """Add network event for cross-reference analysis"""
        self.data_queue.put(('network_event', event_data))
    
    def add_torrent_activity(self, activity_data: Dict):
        """Add torrent activity for cross-reference analysis"""
        self.data_queue.put(('torrent_activity', activity_data))
    
    def add_xcom_intelligence(self, intelligence_data: Dict):
        """Add X.com intelligence for cross-reference analysis"""
        self.data_queue.put(('xcom_intelligence', intelligence_data))
    
    def _process_network_event(self, event_data: Dict):
        """Process network event and store for correlation"""
        self.network_events.append(event_data)
        
        # Store in database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO cross_reference_events 
            (timestamp, event_type, source_system, event_data, correlation_id)
            VALUES (?, ?, ?, ?, ?)
        """, (
            datetime.now().isoformat(),
            'network_event',
            'NetworkSovereigntyMonitor',
            json.dumps(event_data),
            event_data.get('correlation_id', '')
        ))
        
        conn.commit()
        conn.close()
    
    def _process_torrent_activity(self, activity_data: Dict):
        """Process torrent activity and store for correlation"""
        self.torrent_activity.append(activity_data)
        
        # Store in database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO cross_reference_events 
            (timestamp, event_type, source_system, event_data, correlation_id)
            VALUES (?, ?, ?, ?, ?)
        """, (
            datetime.now().isoformat(),
            'torrent_activity',
            'TorrentArchitectureEngine',
            json.dumps(activity_data),
            activity_data.get('correlation_id', '')
        ))
        
        conn.commit()
        conn.close()
    
    def _process_xcom_intelligence(self, intelligence_data: Dict):
        """Process X.com intelligence and store for correlation"""
        self.xcom_intelligence.append(intelligence_data)
        
        # Store in database
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO cross_reference_events 
            (timestamp, event_type, source_system, event_data, correlation_id)
            VALUES (?, ?, ?, ?, ?)
        """, (
            datetime.now().isoformat(),
            'xcom_intelligence',
            'XComIntelligenceParser',
            json.dumps(intelligence_data),
            intelligence_data.get('correlation_id', '')
        ))
        
        conn.commit()
        conn.close()
    
    def _trigger_correlation_analysis(self):
        """Trigger correlation analysis between different data sources"""
        try:
            # Get recent data from database
            conn = sqlite3.connect(self.db_path)
            
            # Get performance data from last hour
            df_performance = pd.read_sql_query("""
                SELECT * FROM system_snapshots 
                WHERE datetime(timestamp) > datetime('now', '-1 hour')
                ORDER BY timestamp
            """, conn)
            
            # Get cross-reference events from last hour
            df_events = pd.read_sql_query("""
                SELECT * FROM cross_reference_events 
                WHERE datetime(timestamp) > datetime('now', '-1 hour')
                ORDER BY timestamp
            """, conn)
            
            conn.close()
            
            if len(df_performance) > 10 and len(df_events) > 0:
                correlations = self._calculate_correlations(df_performance, df_events)
                self._store_correlations(correlations)
                
                # Detect anomalies
                anomalies = self._detect_anomalies(df_performance, df_events)
                self._store_anomalies(anomalies)
                
        except Exception as e:
            print(f"Correlation analysis error: {e}")
    
    def _calculate_correlations(self, df_performance: pd.DataFrame, df_events: pd.DataFrame) -> List[CorrelationResult]:
        """Calculate correlations between performance metrics and events"""
        correlations = []
        
        try:
            # Convert timestamps to datetime
            df_performance['timestamp'] = pd.to_datetime(df_performance['timestamp'])
            df_events['timestamp'] = pd.to_datetime(df_events['timestamp'])
            
            # Resample events to match performance data frequency
            event_counts = df_events.groupby([
                pd.Grouper(key='timestamp', freq='5T'),  # 5-minute bins
                'event_type'
            ]).size().unstack(fill_value=0)
            
            # Merge with performance data
            df_merged = pd.merge_asof(
                df_performance.sort_values('timestamp'),
                event_counts.reset_index().sort_values('timestamp'),
                on='timestamp',
                direction='nearest'
            )
            
            # Calculate correlations between performance metrics and event counts
            performance_cols = ['total_cpu_percent', 'total_memory_percent', 'performance_score']
            event_cols = [col for col in df_merged.columns if col in ['network_event', 'torrent_activity', 'xcom_intelligence']]
            
            for perf_col in performance_cols:
                for event_col in event_cols:
                    if event_col in df_merged.columns and len(df_merged[perf_col].dropna()) > 5:
                        try:
                            correlation, p_value = stats.pearsonr(
                                df_merged[perf_col].fillna(0), 
                                df_merged[event_col].fillna(0)
                            )
                            
                            # Calculate confidence interval
                            n = len(df_merged)
                            stderr = np.sqrt((1 - correlation**2) / (n - 2))
                            ci_low = correlation - 1.96 * stderr
                            ci_high = correlation + 1.96 * stderr
                            
                            # Determine relationship strength
                            abs_corr = abs(correlation)
                            if abs_corr > 0.7:
                                strength = "Strong"
                            elif abs_corr > 0.4:
                                strength = "Moderate"
                            elif abs_corr > 0.2:
                                strength = "Weak"
                            else:
                                strength = "Very Weak"
                            
                            # Generate interpretation
                            direction = "positive" if correlation > 0 else "negative"
                            interpretation = f"{strength} {direction} correlation between {perf_col} and {event_col}"
                            if p_value < 0.05:
                                interpretation += " (statistically significant)"
                            
                            result = CorrelationResult(
                                metric_pair=(perf_col, event_col),
                                correlation_coefficient=correlation,
                                p_value=p_value,
                                confidence_interval=(ci_low, ci_high),
                                relationship_strength=strength,
                                interpretation=interpretation
                            )
                            
                            correlations.append(result)
                            
                        except Exception as e:
                            continue
            
        except Exception as e:
            print(f"Correlation calculation error: {e}")
        
        return correlations
    
    def _store_correlations(self, correlations: List[CorrelationResult]):
        """Store correlation results in database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for correlation in correlations:
            cursor.execute("""
                INSERT INTO correlations
                (timestamp, metric_pair, correlation_coefficient, p_value,
                 confidence_interval, relationship_strength, interpretation)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                f"{correlation.metric_pair[0]} vs {correlation.metric_pair[1]}",
                correlation.correlation_coefficient,
                correlation.p_value,
                json.dumps(correlation.confidence_interval),
                correlation.relationship_strength,
                correlation.interpretation
            ))
        
        conn.commit()
        conn.close()
    
    def _detect_anomalies(self, df_performance: pd.DataFrame, df_events: pd.DataFrame) -> List[PerformanceAnomaly]:
        """Detect performance anomalies using machine learning"""
        anomalies = []
        
        try:
            # Prepare features for anomaly detection
            feature_cols = ['total_cpu_percent', 'total_memory_percent', 'performance_score', 'active_processes']
            
            # Add event counts as features
            df_events['timestamp'] = pd.to_datetime(df_events['timestamp'])
            event_counts = df_events.groupby([
                pd.Grouper(key='timestamp', freq='5T'),
                'event_type'
            ]).size().unstack(fill_value=0)
            
            # Merge performance data with event counts
            df_performance['timestamp'] = pd.to_datetime(df_performance['timestamp'])
            df_features = pd.merge_asof(
                df_performance.sort_values('timestamp'),
                event_counts.reset_index().sort_values('timestamp'),
                on='timestamp',
                direction='nearest'
            )
            
            # Prepare feature matrix
            available_cols = [col for col in feature_cols if col in df_features.columns]
            event_cols = [col for col in df_features.columns if col in ['network_event', 'torrent_activity', 'xcom_intelligence']]
            all_feature_cols = available_cols + event_cols
            
            if len(all_feature_cols) > 0 and len(df_features) > 10:
                X = df_features[all_feature_cols].fillna(0)
                
                # Normalize features
                X_scaled = self.scaler.fit_transform(X)
                
                # Detect anomalies
                anomaly_scores = self.anomaly_detector.fit_predict(X_scaled)
                anomaly_indices = np.where(anomaly_scores == -1)[0]
                
                for idx in anomaly_indices:
                    row = df_features.iloc[idx]
                    
                    # Identify affected metrics (those with extreme values)
                    affected_metrics = []
                    for col in available_cols:
                        if col in row and not pd.isna(row[col]):
                            # Check if value is in top/bottom 10%
                            col_values = df_features[col].dropna()
                            if len(col_values) > 1:
                                percentile_90 = np.percentile(col_values, 90)
                                percentile_10 = np.percentile(col_values, 10)
                                
                                if row[col] > percentile_90 or row[col] < percentile_10:
                                    affected_metrics.append(col)
                    
                    # Calculate severity score
                    severity_score = self._calculate_anomaly_severity(row, df_features)
                    
                    # Find related events
                    timestamp = row['timestamp']
                    related_events = []
                    
                    for event_col in event_cols:
                        if event_col in row and row[event_col] > 0:
                            related_events.append({
                                'type': event_col,
                                'count': row[event_col],
                                'timestamp': timestamp
                            })
                    
                    # Generate recommendations
                    recommendations = self._generate_recommendations(affected_metrics, related_events)
                    
                    anomaly = PerformanceAnomaly(
                        timestamp=timestamp,
                        anomaly_type=self._classify_anomaly_type(affected_metrics),
                        affected_metrics=affected_metrics,
                        severity_score=severity_score,
                        description=f"Performance anomaly detected with severity {severity_score:.2f}",
                        related_events=related_events,
                        recommended_actions=recommendations
                    )
                    
                    anomalies.append(anomaly)
        
        except Exception as e:
            print(f"Anomaly detection error: {e}")
        
        return anomalies
    
    def _calculate_anomaly_severity(self, anomaly_row: pd.Series, df_baseline: pd.DataFrame) -> float:
        """Calculate severity score for detected anomaly"""
        try:
            severity = 0.0
            
            # CPU severity
            if 'total_cpu_percent' in anomaly_row:
                cpu_val = anomaly_row['total_cpu_percent']
                if cpu_val > 90:
                    severity += 0.4
                elif cpu_val > 70:
                    severity += 0.2
            
            # Memory severity
            if 'total_memory_percent' in anomaly_row:
                mem_val = anomaly_row['total_memory_percent']
                if mem_val > 90:
                    severity += 0.3
                elif mem_val > 80:
                    severity += 0.15
            
            # Performance score severity
            if 'performance_score' in anomaly_row:
                perf_val = anomaly_row['performance_score']
                if perf_val < 30:
                    severity += 0.3
                elif perf_val < 50:
                    severity += 0.15
            
            return min(severity, 1.0)
            
        except Exception:
            return 0.5
    
    def _classify_anomaly_type(self, affected_metrics: List[str]) -> str:
        """Classify type of anomaly based on affected metrics"""
        if 'total_cpu_percent' in affected_metrics:
            return "CPU_ANOMALY"
        elif 'total_memory_percent' in affected_metrics:
            return "MEMORY_ANOMALY"
        elif 'performance_score' in affected_metrics:
            return "PERFORMANCE_DEGRADATION"
        else:
            return "GENERAL_ANOMALY"
    
    def _generate_recommendations(self, affected_metrics: List[str], related_events: List[Dict]) -> List[str]:
        """Generate recommendations based on anomaly analysis"""
        recommendations = []
        
        if 'total_cpu_percent' in affected_metrics:
            recommendations.append("Investigate high CPU usage processes")
            recommendations.append("Consider CPU scaling or process optimization")
        
        if 'total_memory_percent' in affected_metrics:
            recommendations.append("Check for memory leaks in running processes")
            recommendations.append("Consider increasing available memory")
        
        if any(event['type'] == 'network_event' for event in related_events):
            recommendations.append("Correlate network activity with performance impact")
            recommendations.append("Check network configuration for efficiency")
        
        if any(event['type'] == 'torrent_activity' for event in related_events):
            recommendations.append("Monitor P2P network impact on system resources")
            recommendations.append("Consider bandwidth throttling for torrent activity")
        
        if not recommendations:
            recommendations.append("Monitor system closely for pattern development")
            recommendations.append("Review system logs for additional context")
        
        return recommendations
    
    def _store_anomalies(self, anomalies: List[PerformanceAnomaly]):
        """Store detected anomalies in database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for anomaly in anomalies:
            cursor.execute("""
                INSERT INTO anomalies
                (timestamp, anomaly_type, affected_metrics, severity_score,
                 description, related_events, recommended_actions)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                anomaly.timestamp.isoformat() if isinstance(anomaly.timestamp, datetime) else str(anomaly.timestamp),
                anomaly.anomaly_type,
                json.dumps(anomaly.affected_metrics),
                anomaly.severity_score,
                anomaly.description,
                json.dumps(anomaly.related_events, default=str),
                json.dumps(anomaly.recommended_actions)
            ))
        
        conn.commit()
        conn.close()
    
    def generate_performance_report(self, hours: int = 24) -> str:
        """Generate comprehensive performance analysis report"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # Get recent data
            cutoff_time = (datetime.now() - timedelta(hours=hours)).isoformat()
            
            # System snapshots
            df_snapshots = pd.read_sql_query("""
                SELECT * FROM system_snapshots 
                WHERE timestamp > ?
                ORDER BY timestamp DESC
            """, conn, params=[cutoff_time])
            
            # Correlations
            df_correlations = pd.read_sql_query("""
                SELECT * FROM correlations 
                WHERE timestamp > ?
                ORDER BY timestamp DESC
            """, conn, params=[cutoff_time])
            
            # Anomalies
            df_anomalies = pd.read_sql_query("""
                SELECT * FROM anomalies 
                WHERE timestamp > ?
                ORDER BY timestamp DESC
            """, conn, params=[cutoff_time])
            
            # Cross-reference events
            df_events = pd.read_sql_query("""
                SELECT event_type, COUNT(*) as count FROM cross_reference_events
                WHERE timestamp > ?
                GROUP BY event_type
            """, conn, params=[cutoff_time])
            
            conn.close()
            
            # Generate report
            report = f"""
üìä PERFORMANCE CROSS-REFERENCE ANALYSIS REPORT
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Analysis Period: Last {hours} hours

=== SYSTEM PERFORMANCE OVERVIEW ===
"""
            
            if len(df_snapshots) > 0:
                avg_cpu = df_snapshots['total_cpu_percent'].mean()
                avg_memory = df_snapshots['total_memory_percent'].mean()
                avg_performance = df_snapshots['performance_score'].mean()
                min_performance = df_snapshots['performance_score'].min()
                max_performance = df_snapshots['performance_score'].max()
                
                report += f"""
Average CPU Usage: {avg_cpu:.1f}%
Average Memory Usage: {avg_memory:.1f}%
Average Performance Score: {avg_performance:.1f}/100
Performance Range: {min_performance:.1f} - {max_performance:.1f}
Total Snapshots Captured: {len(df_snapshots)}
"""
            
            report += f"""
=== CROSS-REFERENCE EVENT ANALYSIS ===
"""
            
            if len(df_events) > 0:
                for _, row in df_events.iterrows():
                    report += f"{row['event_type']}: {row['count']} events\\n"
            else:
                report += "No cross-reference events recorded\\n"
            
            report += f"""
=== CORRELATION ANALYSIS ===
"""
            
            if len(df_correlations) > 0:
                strong_correlations = df_correlations[df_correlations['relationship_strength'].isin(['Strong', 'Moderate'])]
                report += f"Total Correlations Found: {len(df_correlations)}\\n"
                report += f"Strong/Moderate Correlations: {len(strong_correlations)}\\n\\n"
                
                for _, corr in strong_correlations.head(5).iterrows():
                    report += f"‚Ä¢ {corr['metric_pair']}: {corr['correlation_coefficient']:.3f} ({corr['relationship_strength']})\\n"
                    report += f"  {corr['interpretation']}\\n\\n"
            else:
                report += "No significant correlations detected\\n"
            
            report += f"""
=== ANOMALY DETECTION ===
"""
            
            if len(df_anomalies) > 0:
                high_severity = df_anomalies[df_anomalies['severity_score'] > 0.7]
                medium_severity = df_anomalies[df_anomalies['severity_score'].between(0.3, 0.7)]
                
                report += f"Total Anomalies Detected: {len(df_anomalies)}\\n"
                report += f"High Severity (>0.7): {len(high_severity)}\\n"
                report += f"Medium Severity (0.3-0.7): {len(medium_severity)}\\n\\n"
                
                for _, anomaly in df_anomalies.head(3).iterrows():
                    timestamp = anomaly['timestamp']
                    anomaly_type = anomaly['anomaly_type']
                    severity = anomaly['severity_score']
                    
                    report += f"üö® {anomaly_type} at {timestamp}\\n"
                    report += f"   Severity: {severity:.2f}\\n"
                    report += f"   Description: {anomaly['description']}\\n\\n"
            else:
                report += "No performance anomalies detected\\n"
            
            report += f"""
=== PERFORMANCE INSIGHTS ===

üéØ Key Findings:
"""
            
            # Generate insights based on data
            if len(df_snapshots) > 0:
                if avg_cpu > 80:
                    report += "‚Ä¢ High CPU utilization detected - investigate resource-intensive processes\\n"
                if avg_memory > 85:
                    report += "‚Ä¢ High memory usage - consider memory optimization\\n"
                if avg_performance < 60:
                    report += "‚Ä¢ Below-average performance scores - system optimization recommended\\n"
            
            if len(df_correlations) > 0:
                network_correlations = df_correlations[df_correlations['metric_pair'].str.contains('network_event')]
                if len(network_correlations) > 0:
                    report += "‚Ä¢ Network events show correlation with system performance\\n"
            
            report += f"""
=== RECOMMENDATIONS ===

1. Monitor high-impact processes during peak usage periods
2. Correlate network connectivity issues with system performance
3. Analyze torrent P2P activity impact on system resources
4. Implement predictive monitoring based on correlation patterns
5. Set up automated alerts for performance anomalies

=== NEXT STEPS ===

‚Ä¢ Extend monitoring period for trend analysis
‚Ä¢ Implement machine learning models for predictive analytics
‚Ä¢ Integrate with external monitoring systems
‚Ä¢ Create automated response mechanisms for detected anomalies

---
*Generated by Sovereignty Architecture Performance Cross-Reference System v1.0*
"""
            
            return report
            
        except Exception as e:
            return f"Error generating performance report: {e}"
    
    def stop_monitoring(self):
        """Stop performance monitoring"""
        self.monitoring_active = False
        
        # Wait for threads to finish
        for thread in self.monitor_threads:
            if thread.is_alive():
                thread.join(timeout=5)
        
        # Cleanup WMI connection
        if self.wmi_connection:
            try:
                pythoncom.CoUninitialize()
            except:
                pass
        
        print("üìä Performance monitoring stopped")

def main():
    """Main execution function"""
    print("üìä PERFORMANCE CROSS-REFERENCE SYSTEM")
    print("=" * 50)
    
    # Initialize performance system
    perf_system = PerformanceCrossReferenceSystem()
    
    print("üîß Starting performance monitoring...")
    perf_system.start_performance_monitoring()
    
    # Simulate some cross-reference events
    print("üì° Adding sample cross-reference events...")
    
    # Network event
    network_event = {
        'timestamp': datetime.now().isoformat(),
        'event_type': 'REMOTE_DESKTOP_FAILURE',
        'target': 'ATHENA101',
        'status': 'FAILED',
        'correlation_id': 'net_001'
    }
    perf_system.add_network_event(network_event)
    
    # Torrent activity
    torrent_activity = {
        'timestamp': datetime.now().isoformat(),
        'activity_type': 'PEER_DISCOVERY',
        'peer_count': 15,
        'data_transfer': 1024000,
        'correlation_id': 'tor_001'
    }
    perf_system.add_torrent_activity(torrent_activity)
    
    # X.com intelligence
    xcom_intelligence = {
        'timestamp': datetime.now().isoformat(),
        'intelligence_type': 'API_ANALYSIS',
        'endpoints_discovered': 8,
        'tokens_extracted': 3,
        'correlation_id': 'xcom_001'
    }
    perf_system.add_xcom_intelligence(xcom_intelligence)
    
    # Let it run for a bit to collect data
    print("‚è±Ô∏è Collecting performance data for analysis...")
    time.sleep(30)  # 30 seconds of monitoring
    
    # Generate initial report
    print("üìã Generating performance analysis report...")
    report = perf_system.generate_performance_report(hours=1)
    
    print(report)
    
    # Save report
    report_filename = f"performance_analysis_report_{int(time.time())}.txt"
    with open(report_filename, 'w') as f:
        f.write(report)
    
    print(f"\\n‚úÖ Report saved to: {report_filename}")
    print(f"‚úÖ Database: {perf_system.db_path}")
    
    # Stop monitoring
    print("\\nüõë Stopping performance monitoring...")
    perf_system.stop_monitoring()
    
    print("\\nüéØ Performance Cross-Reference System Demo Complete!")
    print("Key capabilities demonstrated:")
    print("  ‚Ä¢ Real-time performance monitoring")
    print("  ‚Ä¢ Cross-reference correlation analysis") 
    print("  ‚Ä¢ Anomaly detection with machine learning")
    print("  ‚Ä¢ Comprehensive reporting and insights")
    print("  ‚Ä¢ Integration with network, torrent, and intelligence systems")

if __name__ == "__main__":
    main()